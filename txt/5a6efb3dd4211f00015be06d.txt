 At a meeting of the United Nations Convention on Conventional Weapons in Geneva in November, a group of experts gathered to discuss the military, legal and ethical dimensions of emerging weapons technologies. Among the views voiced at the convention was a call for a ban on what are now being called “lethal autonomous weapons systems.” A 2012 Department of Defense directive defines an autonomous weapon system as one that, “once activated, can select and engage targets without further intervention by a human operator. This includes human-supervised autonomous weapon systems that are designed to allow human operators to override operation of the weapon system, but can select and engage targets without further human input after activation.” In a sense, autonomous weapons are not new. Land mines, for instance, “select” and engage their own targets once a human has activated them. The Israeli Harpy, a loitering anti-radar missile, deploys without a specifically designated target, flies a search pattern, identifies as an enemy radar and then divebombs and destroys it. But land mines and weapons like the Harpy are not the kinds of autonomous weapons that those lobbying for a ban are specifically concerned about. They are instead concerned about weapons systems of greater technical sophistication that would be used to target humans. As a military ethicist and Iraq war veteran, I, too, worry deeply about the dangers of autonomous weapons, and I am glad that many bright and motivated people are working on this issue. But there are some important problems that so far have been overlooked. Specifically, I worry that the language being used at the highest levels of this debate promotes a conception of autonomous weapons that conflates potential abuses of the technology with features of the technology itself and possesses the danger of absolving programmers and military implementers of moral responsibility where such responsibility is needed most. This is no small matter. The terms in which we frame this debate are crucial: If we fail to understand the problem correctly, and fail to set the appropriate terms and boundaries of the debate from the beginning, we risk searching in the fundamentally wrong places for killer robots and the means to mitigate their pernicious expression. An open letter, signed last year by 116 founders of robotics and artificial intelligence companies calling for a United Nations ban on killer robots states, “Lethal autonomous weapons threaten to become the third revolution in warfare. Once developed, they will permit armed conflict to be fought at a scale greater than ever, and at timescales faster than humans can comprehend. These can be weapons of terror, weapons that despots and terrorists use against innocent populations, and weapons hacked to behave in undesirable ways.” Echoing their sentiments, the Campaign to Stop Killer Robots website calls for a pre-emptive ban on the development, production and use of lethal autonomous weapons. They state, “We are concerned about weapons that operate on their own without meaningful human control. The campaign seeks to prohibit taking the human ‘out-of-the-loop’ with respect to targeting and attack decisions on the battlefield.” The language used here suggests three troubling assumptions; first, that the actual thing to be banned is easily discernible; second, that meaningful human control can be taken “out-of-the loop”; and last, that the goods to be achieved by banning such weapons would make for an overall better world. None of these assumptions are obvious. Taken together, they risk placing moral responsibility onto sophisticated tools rather than on the shoulders of human decision makers. They likewise hold the danger of removing a set of capacities that could be used in defense of justifiable and worthy ends. Let’s begin with a basic fact. The capacities of autonomous weapons are designed by humans, programmers whose intentions are written into the system software. Let us remember, though, that the essence of computation needn’t be restricted to a silicon substrate or to any metaphysically privileged medium. Imagine that rather than having a programmer’s intentions written into a robot’s silicon substrate, we instead have a group of people physically carrying out the identical set of decisions and procedures. Given such a case, the decision-procedure being realized by the group of people would be functionally identical to the decision-procedure being realized on silicon. Supposing such a case, we might then ask, would an army realizing an institutional plan that was functionally identical to the algorithms of a killer robot be any less morally problematic? Would we want to then ban that particular army arrangement as well? If so, how? The main point here is to emphasize the serious problem policymakers would face in isolating just what exactly it is that should be banned. Furthermore, such considerations highlight the fact that the deep moral issues raised by autonomous weapons are the very same ones raised by conventional warfare. When we start to look at killer robots this way, we see that they are the mechanical realizations of much larger sets of institutional intentions, involving the plans of designers and the decisions of people who follow such plans, individuals who accept certain jobs, develop certain software and put such software to military use. Indeed, there is the problem of determining who exactly within such a causal chain is morally responsible for potential harm and to what degree. However, the problem of determining where specifically within a given causal chain to assign moral responsibility to individual members is a problem endemic to any collective action whatsoever, from the Challenger disaster, to the BP oil spill, to global poverty. The current language in the killer robot debate suggests that those weapons are capable of acting without meaningful human control, and that their creation and use is somehow distinct from other sorts of collective actions. It also suggests that potential harm arising from that creation and use may be morally unattributable to those who create and use them. This is not the sort of moral detachment we should foster in our technology and military communities, especially in relation to what is perhaps the gravest and most consequential of all human activities: war. One common response to this view is that A.I. experts are particularly worried about learning machines, and that something about the complexity of these systems gives rise to emergent properties that are metaphysically irreducible to the sum of their parts. This would suggest that the robots possess an agency, and should then be subject to moral accountability, with the accompanying rights and interests. I am skeptical of this. If killer robots are truly, as I argue, expressions of complex institutional arrangements of humans, and thus proxies for our own collective intentionality, then programmers, implementers and military decision makers alike need to rigorously examine their own relationships to these institutions, and the consequences of their engagement with them. Aleksandr Solzhenitsyn once wrote that “the line separating good and evil passes not through states, nor between classes, nor between political parties either — but right through every human heart — and through all human hearts.” Accordingly, the “off switch” to mitigate the unjust harm that killer robots may cause is in each one of us, at every moment and in every decision, big and small. This has always been the case.