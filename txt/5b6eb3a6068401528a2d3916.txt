Years ago I spoke with a 16-year-old girl who was considering the idea of having a computer companion in the future, and she described the upside to me. It’s not that the robot she’d imagined, a vastly more sophisticated Siri, was so inspiring. It’s that she’d already found people to be so disappointing. And now, for the first time, she explained me, people have options. Back then I thought her comments seemed prescient. Now I find them timely. “There are people who have tried to make friends, but stumbled so badly that they’ve given up,” she said. “So when they hear this idea of robots as companions, well … it’s not like a robot has the mind to walk away or leave you or anything like that.” This girl had grown up in the time of Siri, a conversational object presented as an empathy machine — a thing that could understand her. And so it seemed natural to her that other machines would expand the range of conversation. But there is something she may have been too young to understand — or, like a lot of us — prone to forget when we talk to machines. These robots can perform empathy in a conversation about your friend, your mother, your child or your lover, but they have no experience of any of these relationships. Machines have not known the arc of a human life. They feel nothing of the human loss or love we describe to them. Their conversations about life occupy the realm of the as-if.  Yet through our interactions with these machines, we seem to ignore this fact; we act as though the emotional ties we form with them will be reciprocal, and real, as though there is a right kind of emotional tie that can be formed with objects that have no emotions at all.   In our manufacturing and marketing of these machines, we encourage children to develop an emotional tie that is sure to lead to an empathic dead end. On top of this, it has become fashionable for psychologists to critique empathy, a unique form of human connection, just at a time when we are embarking on relationships with objects with none to give. The coincidence is too convenient: Children will lose the ability to have empathy if they relate too consistently with objects that cannot form empathic ties.  Technology challenges us to look at our human values. We can try to use technology to cure Parkinson’s or Alzheimer’s, which would be a blessing, but that blessing is not a reason to move from artificial brain enhancement to artificial intimacy.  And yet that is the kind of talk that one hears these days. The narrative begins with the idea that companionate robots would be “better than nothing,” better because there aren’t enough people to teach, love and tend to people. But that idea quickly shifts into another: robots would be better than most anything. Unlike people, they would not abandon you or get sick and die. They might not be capable of love, but they won’t break your heart. From better than nothing to better than anything. These are stations on our voyage to forgetting what it means to be human. But the forgetting begins long before we have a robot companion in place; it begins when we even think of putting one in place. To build the robots, we must first rebuild ourselves as people ready to be their companions. In early May, I was filmed for a documentary about technology and humanity. The director, Bennett Miller, shows me video clips from interviews he’s done with some of the world’s most famous futurists. Danny Hillis, Ray Kurzweil, Kevin Esvelt. Again and again I hear: There are not enough people to care for the elderly; naturally, robots will do that job. There is an epidemic of loneliness; robots will make this a thing of the past. People crave immortality; our re-embodiment in machines will make that possible. And above all, this: It is our nature, our human nature, to evolve toward our maximum potential; it is our human destiny to evolve into the superior cognitive beings of a robotic future.  I’ve heard it before, but still I’m rattled. I stand accused by my happy technologist colleagues and their unrelenting enthusiasm. Accused of what? Species chauvinism, I suppose. My colleagues make assumptions about the future of being human with which I am not comfortable. First, that it is our human nature to want to evolve toward being more than human and, ultimately, immortal. The steps to get there are small. We will want to cure brain diseases. And from there, we will want to have superior brains. And from there, we will want our brains to live forever. I am an outsider to these cheerful pronouncements, which are not so much a conversation about human values as a technological ideology about what posthuman values should be. Ironically, to deny the need for death is to deny the humanness of having real conversations about it. Technologists presented us with artificial intelligence, and in the end it made us look differently, and more critically, at the kind of intelligence that only people have. It is an intelligence that is tempered with knowledge of our bodies, of our place in families and history. Now, science goes a step further and presents us with artificial intimacy, yet another form of A.I. Again, this is an intimacy that does not make room for human empathy or what human beings in their bodies experience as the fear of death, loneliness, illness, pain. We diminish as the seeming empathy of the machine increases. It is technology forcing us to forget what we know about life. In life, you are struck by the importance of presence, of the small moments of meaning, the miracle of your child’s breath, the feelings of deep human connection. When you are thinking about technology, your mind is not on all of that. We program machines to appear more empathic. Being human today is about the struggle to remain genuinely empathic ourselves. To remember why it matters, to remember what we cherish.  These days, to be human is to keep one’s mind on the glory that one is. Sherry Turkle is a professor in the program in Science, Technology and Society at M.I.T. She is the author of “Reclaiming Conversation” and “Alone Together,” both of which deal with a critique of sociable robotics. Now in print: “Modern Ethics in 77 Arguments,” and “The Stone Reader: Modern Philosophy in 133 Arguments,” with essays from the series, edited by Peter Catapano and Simon Critchley, published by Liveright Books. Follow The New York Times Opinion section on Facebook and Twitter, and sign up for the Opinion Today newsletter.