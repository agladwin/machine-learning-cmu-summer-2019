Suppose you need to see a dermatologist. Your friend recommends a doctor, explaining that “she trained at the best hospital in the country and is regarded as one of the top dermatologists in town.” You respond: “How wonderful. How do you know her?” Your friend’s answer: “We met at the Republican convention.” Knowing a person’s political leanings should not affect your assessment of how good a doctor she is — or whether she is likely to be a good accountant or a talented architect. But in practice, does it? Recently we conducted an experiment to answer that question. Our study, done with the researchers Joseph Marks, Eloise Copland and Eleanor Loh for the journal Cognition, found that knowing about people’s political beliefs did interfere with the ability to assess those people’s expertise in other, unrelated domains. In our experiment, we assigned people the most boring imaginable task: to sort 204 colored geometric shapes into one of two categories, “blaps” and “not blaps,” based on the shape’s features. We invented the term “blap,” and the participants had to try to figure out by trial and error what made a shape a blap. Unknown to the participants, whether a shape was deemed a blap was in fact random. First, the participants received feedback about whether their answers were right. (Because answers were deemed to be correct at random, their success rate was around 50 percent.) They also observed the answers of four other “co-players” who were completing the same task. The co-players were actually computer algorithms designed to appear to perform the task with various levels of proficiency. At the same time the participants were also asked whether they agreed or disagreed with a large number of statements about politics — for example, “Building a wall along the southern border would reduce illegal immigration.” They also observed the responses of the other co-players (again, algorithms), which appeared to vary in political outlook. Some of the co-players were really good at identifying blaps; some were not good at all. Some mostly agreed with the participants on politics; some mostly disagreed. As a result, a co-player could be, for example, really good at the task but politically dissimilar to a participant, or really bad at the task but have the same political views. Then came the important part. The participants were shown a new set of shapes and were paid for correctly categorizing them. To help them out, we offered them the opportunity, on each trial, to observe the response of one or two other co-players before reaching their decision. To make the most money, the participants should have chosen to hear from the co-player who had best demonstrated an ability to identify blaps, regardless of that co-player’s political views. But in general, the participants did not do this. Instead, they most often chose to hear about blaps from co-players who were politically like-minded, even when those with different political views were much better at the task. In addition to choosing more often to hear from co-players who were politically like-minded, when making their decisions about whether a shape was a blap, participants were also more influenced by politically like-minded co-players than co-players with opposing political views. In short, people sought and then followed the advice of those who shared their political opinions on issues that had nothing to do with politics, even when they had all the information they needed to understand that this was a bad strategy. Why? This may be an example of what social scientists call the halo effect: If people think that products or people are good along one dimension, they tend to think that they are good along other, unrelated dimensions as well. People make a positive assessment of those who share their political convictions, and that positive assessment spills over into evaluation of other, irrelevant characteristics. Our findings have obvious implications for the spread of false news, for political polarization and for social divisions more generally. Suppose that someone with identifiable political convictions spreads a rumor about a coming collapse in the stock market, a new product that supposedly fails, cheating in sports or an incipient disease epidemic. Even if the rumor is false, and even if those who hear it have reason to believe that it is false, people may well find it credible (and perhaps spread it further) if they share the political views of the source of the rumor. Our results also suggest some harmful consequences of political polarization. Suppose that people trust those who are politically like-minded, even on subjects on which they are clueless. Suppose that they distrust those with different political opinions on nonpolitical issues where they have real expertise. If so, the conditions are ripe for a host of mistakes — and not just about blaps.